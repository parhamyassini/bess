# Copyright (c) 2014-2017, The Regents of the University of California.
# Copyright (c) 2016-2017, Nefeli Networks, Inc.
# All rights reserved.
#
# Description: Full mDC agent including file_reader, file_writer and mdc_agent for making an
#  autonomous agent.


import mdcpkts

# Agent IDs we use: Agent1= 4, Agent2= 16, Agent3= 64
AGENT_ID = 4
DEFAULT_SESSIONS_1 = [{'addr': '06:16:3e:1b:72:31', 'label': 0x50505054}]
DEFAULT_SESSIONS_2 = [{'addr': '06:16:3e:1b:72:32', 'label': 0x50505054}]
DEFAULT_SESSIONS_3 = [{'addr': '06:16:3e:1b:72:33', 'label': 0x50505054}]

# Ping-pong protocol support
PING_ENABLED = False 
PING_PKT_RATE = 1e3
PING_SRC_MAC = 'F8:F2:1E:3A:13:C4'
PING_DST_MAC = '02:53:55:4d:45:00'

# NUMA and MQ support

# Number of processors to be used
#NUM_PROCS = 1
# Available cores per processor
NUM_CORES = 10 #20
# Number of NIC queues for the agent.
# If TGEN_ENABLED is True, the script will use two extra queues for traffic generation
NUM_QUEUES = 4
NIC_PCI_IDS = {
               0: [],
               1: [2, 3], #, 4, 5] #['0000:81:00.0', '0000:81:00.1']
               }
# CPU layout
CORE_IDS = {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
                20, 21, 22, 23, 24, 25, 26, 27, 28, 29],

            1: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
                30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
            }


NUM_PROCS = len(CORE_IDS)

# Spark support
SPARK_ENABLED = int($MDC_ENABLE_SPARK!False)
SPARK_RAMDISK_PATH = "/mnt/ramdisk/"
SPARK_SOCKET_PATH = "/home/hadoop/domain_socket_file"

# Traffic Generator: Use *ONLY* for stress test
TGEN_ENABLED = False 
# in Gbps
TGEN_THROUGHPUT = 10
TGEN_PKT_SIZE = 64 

# End of configuration

mq_enabled = NUM_QUEUES > 1
# Set NUM_CORES to the min. number of cores per processor
curr_num_cores = NUM_CORES

for p_idx in range(NUM_PROCS):
    num_cores = len(CORE_IDS[p_idx])
    if num_cores < curr_num_cores:
        curr_num_cores = num_cores

NUM_CORES = curr_num_cores

worker_info_map = {}
proc_worker_map = {}
next_wid = 0
first_wid = -1
for p_idx in NIC_PCI_IDS:
    proc_cores = CORE_IDS[p_idx][:NUM_CORES]
    for c_idx, c_id in enumerate(proc_cores):
        #w_id = p_idx * NUM_PROCS + c_idx
	#print(p_idx, c_id, next_wid)
        worker_info_map[next_wid] = {'core': c_id, 'core_idx': c_idx, 'proc': p_idx}
        if p_idx not in proc_worker_map:
            proc_worker_map[p_idx] = []
        proc_worker_map[p_idx].append(next_wid)
        bess.add_worker(wid=next_wid, core=c_id)
        next_wid += 1
	if first_wid == -1:
		first_wid = next_wid

# Define Ports and Queues
src_count = NUM_QUEUES
ip_list = [
('7.38.233.147', '55.169.157.31'),
('208.97.72.186', '157.28.205.149'),
('117.199.131.176', '79.219.231.247'),
('213.151.109.0', '44.122.57.30'),
('138.101.141.33', '197.122.187.56'),
('228.93.108.53', '64.211.103.156'),
('178.204.3.28', '4.63.60.250'),
('16.123.19.226', '10.199.212.27'), 
]
mdc_rec_count = 0
mdc_rec_port = {}
port_count = 0

for p_idx, nic_list in NIC_PCI_IDS.items():
    next_wid_idx = 0
    for nic_idx, nic_pci_id in enumerate(nic_list):
        num_queues = NUM_QUEUES+src_count if TGEN_ENABLED else NUM_QUEUES
        port_name = 'port_%d_%d' % (p_idx, nic_idx)
        port = PMDPort(name=port_name, port_id=nic_pci_id, num_inc_q=NUM_QUEUES, num_out_q=num_queues, size_inc_q=4*1024, size_out_q=4*1024)
        port_count += 1

        # Create inc/out queues for the agent
        for q_idx in range(NUM_QUEUES):
            q_inc = 'q_inc_%s_%d' % (port_name, q_idx)
            q_out = 'q_out_%s_%d' % (port_name, q_idx)
            QueueInc(name=q_inc, port=port, qid=q_idx)
            QueueOut(name=q_out, port=port, qid=q_idx)

            # Assign queues to workers
            next_wid = proc_worker_map[p_idx][next_wid_idx]
            bess.attach_task(module_name=q_inc, wid=next_wid)

            # Store the mdc_rec_count and (q_inc, q_out) mapping
            mdc_rec_port[mdc_rec_count] = (q_inc, q_out, next_wid)
            mdc_rec_count += 1

            if next_wid_idx < len(proc_worker_map[p_idx]) - 1:
                next_wid_idx += 1
            else:
                next_wid_idx = 0

        if TGEN_ENABLED: # and nic_idx == 0:
            for src_idx in range(src_count):
                src_ip = ip_list[src_idx % len(ip_list)][0]
                dst_ip = ip_list[src_idx % len(ip_list)][1]
                # Creates two output queues
                q_out_name = 'q_out_%s_%d' % (port_name, NUM_QUEUES + src_idx)
                q_out_tgen = QueueOut(name=q_out_name, port=port_name, qid=NUM_QUEUES+src_idx)

                tgen_pkt_rate = (1./float(src_count)) * TGEN_THROUGHPUT * 1e9 / (8 * (TGEN_PKT_SIZE + 24))
		print(tgen_pkt_rate)
                tgen_tc_name = 'tgen%d_%d_%d' % (src_idx, p_idx, nic_idx)
                next_wid = proc_worker_map[p_idx][next_wid_idx]
                bess.add_tc(tgen_tc_name, policy='rate_limit',
                            resource='packet', limit={'packet': int(tgen_pkt_rate)}, wid=next_wid)

                # Creates a source, and limits its pkt rate
                tgen_src = Source()
                tgen_src.attach_task(parent=tgen_tc_name)

                if next_wid_idx < len(proc_worker_map[p_idx]) - 1:
                    next_wid_idx += 1
                else:
                    next_wid_idx = 0
		print(src_ip, dst_ip)
                unlabeled_data_pkt = mdcpkts.make_mdc_unlabeled_data_pkt(label=0x000000,
                                                                         pkt_len=TGEN_PKT_SIZE,
                                                                         src_mac='02:1e:67:9f:4d:ae',
                                                                         dst_mac='06:16:3e:1b:72:32',
                                                                         ip_encap=mq_enabled,
                                                                         src_ip=src_ip,
                                                                         dst_ip=dst_ip)
                unlabeled_data_pkt_bytes = bytes(unlabeled_data_pkt)
		#unlabeled_data_pkt.show()

                # Sends generated traffic on Port1
                tgen_src -> Rewrite(templates=[unlabeled_data_pkt_bytes]) -> q_out_tgen

print('This is the full MDC agent script')

print('Processor count = %d' % NUM_PROCS)
print('Core count per processor = %d' % NUM_CORES)
print('Number MDC Receivers = %d' % mdc_rec_count)
print('Number of ports = %d' % port_count)
print('Multi-queue enabled = %d' % mq_enabled)
print('Number of Queues per port = %d' % NUM_QUEUES)
print('Ping enabled = %d' % PING_ENABLED)
print('Spark enabled = %s' % SPARK_ENABLED)

# Creates mDC modules: a receiver and control pkt generator
#throughput = AwsMdcThroughput()
#merger = Merge(name='merger')

for mdc_rec_idx in mdc_rec_port:
    q_inc, q_out, rec_wid = mdc_rec_port[mdc_rec_idx]
    print(q_inc)
    mdc_rec_name = 'mdc_rec_%d' % mdc_rec_idx
    rec = MdcReceiver(name=mdc_rec_name,
                      ip_encap=mq_enabled,
                      agent_id=AGENT_ID,
                      agent_label=AGENT_ID,
                      switch_mac='00:00:00:00:00:00',
                      agent_mac='00:00:00:00:00:00')
    # Adds (session address, label) tuples to the MdcReceiver module
    #if mdc_rec_idx in [0, 1]:
    #    rec.add(entries=DEFAULT_SESSIONS_2)
    out_wid = 39
    if mdc_rec_idx in range(0, 16): #[8, 9, 10, 11, 12, 13, 14, 15]:
        rec.add(entries=DEFAULT_SESSIONS_1)
        out_wid = rec_wid #36 + int(mdc_rec_idx/4)
        print('XXX', mdc_rec_idx, out_wid)
        #if mdc_rec_idx in range(8, 9):
        #    out_wid = 36
        #else:
        #    out_wid = 37
    #if mdc_rec_idx in range(0, 8):
    #    out_wid = 19
    #if mdc_rec_idx in [4, 5]:
    #    rec.add(entries=DEFAULT_SESSIONS_3)

    # packets from the incoming port on ext_gate, are sent to mdc_receiver
    # port0_inc0 -> 1:mdc_receiver
    tmp_sink_rec1_name = 'sink_rec1_%d' % mdc_rec_idx
    s1 = Sink(name=tmp_sink_rec1_name)
    #tmp_q_inc_name = 'tmp_q_inc_%d' % mdc_rec_idx
    #tmp_q_inc = Queue(name=tmp_q_inc_name, size=4096)
    #bess.connect_modules(q_inc, tmp_q_inc_name)
    bess.connect_modules(q_inc, mdc_rec_name, igate=1)
    #bess.attach_task(module_name=tmp_q_inc_name, wid=rec_wid)
    #bess.connect_modules(q_inc, tmp_sink_rec1_name)
    #bess.connect_modules(q_inc, q_out)

    # Processed pkts: unlabeled pkts and update-state ctrl pkts (port0_out0 is TOR output from mdc_receiver)
    # mdc_receiver:0  -> Queue() -> port0_out0
    tmp_out_q_name = 'tmp_out_q_%d' % mdc_rec_idx
    tmp_out_q = Queue(name=tmp_out_q_name, size=2048)
    bess.connect_modules(mdc_rec_name, tmp_out_q_name)
    #bess.connect_modules(tmp_out_q_name, tmp_sink_rec1_name)
    bess.connect_modules(tmp_out_q_name, q_out)
    bess.attach_task(module_name=tmp_out_q_name, wid=out_wid)

    if not SPARK_ENABLED:
	tmp_sink_rec_name = 'sink_rec_%d' % mdc_rec_idx
        tmp_sink_rec = Sink(name=tmp_sink_rec_name)
        bess.connect_modules(mdc_rec_name, tmp_sink_rec_name, ogate=1)
        #bess.connect_modules(tmp_sink_q_name, 'merger', igate=mdc_rec_idx)
        
        # if mq_enabled:
        #     bess.connect_modules(tmp_sink_q, sink)
        # else:
        #     throughput = AwsMdcThroughput()
        #     bess.connect_modules(tmp_sink_q, throughput)
        #     bess.connect_modules(throughput, sink)

    # Ping pkts (only for the first receiver)
    if PING_ENABLED and mdc_rec_idx == 0:
        ping_pkt = mdcpkts.make_mdc_ping_pkt(agent=AGENT_ID,
                                             src_mac=PING_SRC_MAC,
                                             dst_mac=PING_DST_MAC,
                                             ip_encap=mq_enabled)
        ping_pkt_bytes = bytes(ping_pkt)
        mdc_pkt_gen::MdcPktGen(template=ping_pkt_bytes, pps=PING_PKT_RATE)
        ping_q = Queue(name='ping_q')
        bess.connect_modules('mdc_pkt_gen', 'ping_q')
        bess.connect_modules('ping_q', mdc_rec_name, igate=2)
        ping_q.attach_task(wid=first_wid)
        mdc_pkt_gen.attach_task(wid=first_wid)


#merger -> Queue() -> throughput -> Sink()

if SPARK_ENABLED:
    # Modules for communication with Spark
    stream_socket = UnixStreamSocketPort(name='stream_socket', path=SPARK_SOCKET_PATH)
    # TODO @parham: Check this line, template should be hdr or complete packet?
    file_reader_template = mdcpkts.mdc_unlabeled_data_hdr(label=0x0A0B0C0D)
    file_reader::SignalFileReader(template=file_reader_template)
    file_writer::FileWriter(write_path=SPARK_RAMDISK_PATH, hdr_len=len(file_reader_template))
    spark_interface::SparkInterface(spark_gate=0, file_gate=1, hdr_len=len(file_reader_template))

    # TODO: we need to rethink the MQ support and Spark support
    # Interfacing with spark
    PortInc(port='stream_socket') -> 0:spark_interface:0 -> PortOut(port='stream_socket')
    spark_interface:1 -> file_reader -> 0:mdc_receiver

    # If the Agent host has a receiver, sends these pkts to file writer
    # TODO @parham File reader needs to be connected to 1:mdc_receiver
    mdc_receiver:1 -> file_writer -> 1:spark_interface

# Health check pkts
q::Queue()
mdc_pkt_gen -> q -> 2:mdc_receiver
q.attach_task(wid=num_cores-1)
mdc_pkt_gen.attach_task(wid=num_cores-1)